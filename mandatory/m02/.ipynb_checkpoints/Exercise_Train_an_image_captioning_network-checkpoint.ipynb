{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Train an image captioning network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"*Image Captioning is the process of generating textual description of an image. It uses both Natural Language Processing and Computer Vision to generate the captions*\"\n",
    "\n",
    "In this mandaroty exercise you are implementing an image captioning network. The network will consist of an encoder and a decoder. The encoder is a convolutional neural network, and the decoder is a recurrent neural network. Producing reasonable textual description of an image is a hard task, however with the use of a CNN and a RNN we can start to generate somewhat plausible descriptions. \n",
    "\n",
    "\n",
    "Links:\n",
    "- [Task1: Implementation](#Task1)\n",
    "- [Task2: Train the recurrent neural network](#Task2)\n",
    "- [Task3: Generate image captions](#Task3)\n",
    "\n",
    "\n",
    "Software version:\n",
    "- Python 3.6\n",
    "- Pytorch 1.0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluation format ###\n",
    "\n",
    "\n",
    "You will be guided through the implementation step by step, and you can check your implementation at each step. Note, you will often need to complete all previous steps in order to continue.\n",
    "\n",
    "In the implementation part, test functions allow you to check your code rapidly. When your code works, you can apply it on part 2 to train and part 3 to generate captions. Part 2, the training process, is slow and can run over night on a typical laptop computer. \n",
    "\n",
    "There is no acceptance criterion on part 2 and 3. Your task is however to train a couple of models and to generate plausible captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Delivery ###\n",
    "\n",
    "You are to deliver the zip file created by \"collectSubmission.sh\".\n",
    "If you donâ€™t want to use the script, you can zip the files yourself. Please do not include the following folders to reduce the size of the zip file:\n",
    "    \n",
    "- data\n",
    "- storedModels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise content\n",
    "\n",
    "\n",
    "All subtasks that you are to answer are found in this notebook. All implementation should be done in the file \"cocoSource.py\" found in folder \"/sourceFiles/\". The skeleton of the program is already implemented and contains things such as:\n",
    "- Importing data\n",
    "- Training framework\n",
    "- Saving and restoring models\n",
    "\n",
    "\n",
    "As mentioned, an image captioning network consists of an encoder and a decoder. Your task is to implement the decoder (RNN). The images have already been processed through a CNN. The feature vectors are stored in pickle files together with corresponding labels.\n",
    "\n",
    "\n",
    "During task 1, you will implement all required functionalities for training the image captioning network. In task 2, you will train the network and study how different RNN arcitectures influences the loss. You will generate image captions from images in the validation set in task 3. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dataset ###\n",
    "\n",
    "We will use a dataset called \"Common Object in Context\" (COCO) 2017. It has ~120,000 training and 5,000 validation images. Every image also includes ~5 captions.\n",
    "\n",
    "![](utils_images/bear.png)\n",
    "\n",
    "Captions:\n",
    "\n",
    "- A big burly grizzly bear is show with grass in the background.\n",
    "- The large brown bear has a black nose.\n",
    "- Closeup of a brown bear sitting in a grassy area.\n",
    "- A large bear that is sitting on grass. \n",
    "- A close up picture of a brown bear's face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Network architecture ###\n",
    "\n",
    "**Encoder**\n",
    "\n",
    "\n",
    "Convolutional neural networks have shown to be useful for extracting high level features from images. We will use a pretrained VGG16 network trained on ImageNet. ImageNet consists of 1,2 million images distributed over 1000 classes, and we hope the model have learnt many general features. We are not interested in classifying the 1000 classes, and will change the last fully connected layer with our own. We will use a tanh actiation function to squeeze the values between -1 and 1 similar to the recurrent cells. \n",
    "\n",
    "**Decoder**\n",
    "\n",
    "To be able to convert the high level features from the encoder to natural language, we will construct a recurrent neural network. The output of the encoder will be passed as the initial state to the recurrent cells. The input to the recurret neural network will be word embeddings which we will learn. \n",
    "\n",
    "**Loss function**\n",
    "\n",
    "The words will be considered as separate classes and we shall use cross entropy loss.\n",
    "\n",
    "**Training vs testing**\n",
    "\n",
    "When we train the RNN, we will feed in the correct token (word) for every time step, see figure 2a. The words (tokens) are generally unknown, and in test mode, we will need to use our best estimate as input. Example, if the word \"brown\" has the highest probabillity after the softmax at timestep 2, we will feed in \"brown\" as input at timestep 3, see figure 2b.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/image_captioning_diagram_train.png\", width=2200>\n",
       "Figure 2a: The figure shows an example with 3 recurrent cells stacked in train mode.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/image_captioning_diagram_train.png\", width=2200>\n",
    "Figure 2a: The figure shows an example with 3 recurrent cells stacked in train mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/image_captioning_diagram_test.png\", width=2200>\n",
       "Figure 2b: The figure shows an example with 3 recurrent cells stacked in test mode.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/image_captioning_diagram_test.png\", width=2200>\n",
    "Figure 2b: The figure shows an example with 3 recurrent cells stacked in test mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Vocabulary ###\n",
    "\n",
    "\n",
    "To make good decisions in terms of the network architecture, we will need to know the statistics of the captions:\n",
    "- The number of words in the captions (sequence length) should be considered when chooseing the truncated backpropagation length. \n",
    "- To save memory, it is normal to limit the vocabulary size/length. There will be a tradeoff between capturing all words and have a reansonble sized softmax layer. \n",
    "\n",
    "\n",
    "Note: The captions have been filtered such that all special characters have been removed. This includes also punctuations and commas. All characters have been changed to lower case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "**Vocabulary size**\n",
    "\n",
    "To make good predictions we need the model to have the abillity to predict frequent words. Figure 3 shows a sorted histogram of the word count for the different words. We can see that the majority of the words are within the 2000 most frequent words. Figure 4 shows the precentage of the words accounted for as a function of the vocabulary size.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/Word_count_hist.png\", width=800>\n",
       "*Figure 3*\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/Word_count_hist.png\", width=800>\n",
    "*Figure 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/accumulated_Word_count.png\",width=800>\n",
       "*Figure 4*\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/accumulated_Word_count.png\",width=800>\n",
    "*Figure 4*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Truncated backpropagation length**\n",
    "\n",
    "\n",
    "The sequence length of the recurrent neural network should be able to capture the time(step) dependencies within the captions. Figure 5 shows a histogram of the caption counts as a function of the caption (/sequence) length. Figure 6 shows the precentage of all captions with shorter or equal caption length as a function of caption (/sequence) length. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/Sequence_lengths_hist.png\", width=800>\n",
       "*Figure 5*\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/Sequence_lengths_hist.png\", width=800>\n",
    "*Figure 5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/accumulated_Sequence_length_count.png\", width=800>\n",
       "*Figure 6*\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/accumulated_Sequence_length_count.png\", width=800>\n",
    "*Figure 6*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Common words and their tokens**\n",
    "\n",
    "\n",
    "Every word is assosiated with a token. The words are sorted such that the most frequent words have lower token values. In the table below, you find the six most common words in the dataset. The three first words/tokens have a spescial purpuse:\n",
    "- \"eeee\": Indicates the end of a caption.\n",
    "- \"ssss\": Start token, first word \n",
    "- \"UNK\": As the vocabulary size often is smaller than the number of words in the datasset, we change all \"unknown\" words to the word \"UNK\"\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/vocabulary.png\", width=800>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/vocabulary.png\", width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "### Word embedding  ###\n",
    "\n",
    "\n",
    "The input to the first rnn layer is the embedded version of the input words. We will define a word embedding matrix with shape [vocabulary_size, embedding_size)]. A single word embedding will be a row vector with length [embedding_size]. The tokens are used to select the correct row within the word embedding matrix. For example, the word \"a\" will have values in the third row in the embedding matrix (where first row is row zero).\n",
    "\n",
    "\n",
    "a_emb = wordEmbedding[a_token, :]\n",
    "\n",
    "\n",
    "The word embedding matrix will be initialized with random values and they will be updated as part of the training process. The goal is that similar words get similar vector representations within the embedding matrix. This will not be part of the assigment, but as a test, the vector representations could be embedded using e.g. t-SNE for plotting in 2D/3D space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Data preparation  ###\n",
    "\n",
    "Before starting to work on implementing the network aritecture, you should check if you need to download and process data:\n",
    "\n",
    "- **UIO**: If you work on a IFI computer or one of the ML servers, we have already processed and made the data avaiable for you. The paths are given in the notebook.\n",
    "\n",
    "- **Personal computer**: If you plan to work on the assigment on your personal computer, you will need to download and process the data yourself. Follow the steps in notebook \"data_preparation.ipynb\". Running the data preparation can take several hours. \n",
    "\n",
    "*Change the configuration below if you use CPU or GPU*\n",
    "\n",
    "\n",
    "\n",
    "The processed data are pickle files holding the following information for every image:\n",
    "- Path to the jpg image\n",
    "- Captions\n",
    "- Captions given as tokens\n",
    "- The fc7 output from the VGG16 network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Task1'></a>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Task 1: Implementation #\n",
    "See function headings for specification of each function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** General advice**\n",
    "\n",
    "Do not regenerate the data if you don't need it, it will run overnight on a laptop without GPU. Use either /opt/ifi/anaconda3/bin/jupyter notebook or the ml-servers. \n",
    "\n",
    "Part 1 is for developing your code and runs fairly fast. \n",
    "Part 2 does the actual captioning and can take some hours on CPU. \n",
    "\n",
    "\n",
    "Restarting the notebook after each subtask can be a good idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1a** \n",
    "\n",
    "Your are to implement a vanailla RNN class in the function RNNCell in file sourceFiles/cocoSource.py. The class shall have a constructor and the function \"forward\". To speed up inference, we will concatinate the input and the old state matrices and perform a single matrix multiplication. The difference is illustrated below.\n",
    "\n",
    "Vanilla RNN:\n",
    "\n",
    "\\begin{align}\n",
    "h_t &= tanh(x_tW^{hx} + h_{t-1}W^{hh} + b) \\\\\n",
    "\\\\\n",
    "h_t &= tanh([x_t,h_{t-1}]W + b)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "tmp type torch.FloatTensor\n",
      "The \"weight\" matrix has correct shape\n",
      "The \"bias\" matrix has correct shape\n",
      "The \"weight\" matrix has correct values\n",
      "The \"bias\" matrix has correct values\n",
      "cpu\n",
      "tmp type torch.FloatTensor\n",
      "The \"forward\" function is implemented incorrectly\n"
     ]
    }
   ],
   "source": [
    "#Import modules\n",
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "import torch\n",
    "\n",
    "#Defining dummy variables\n",
    "hidden_state_sizes = 512\n",
    "inputSize  = 256\n",
    "batch_size = 128\n",
    "x          = torch.tensor(torch.rand(batch_size, inputSize))\n",
    "state_old  = torch.tensor(torch.rand(batch_size, hidden_state_sizes))\n",
    "\n",
    "# You should implement this function\n",
    "cell   = cocoSource.RNNCell(hidden_state_sizes, inputSize)\n",
    "out = cell(x, state_old)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.RNNcell_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1b** \n",
    "\n",
    "Your are to implement a Gated recurrent units (GRU) class in the function GRUCell in cocoSource.py. The class shall have a constructor and the function \"forward\". To speed up inference, we will concatinate the input and the old state matrices and perform a single matrix multiplication. The difference is illustrated in the equations below, where you are to implement the equations in the column to the right.\n",
    "\n",
    "GRU:\n",
    "\n",
    "\\begin{align}\n",
    "&Update \\: gate: \\qquad &\\Gamma^u=\\sigma(x_tW^{u} + h_{t-1}U^{u} + b^u) \\qquad \\rightarrow \\qquad &\\Gamma^u=\\sigma([x_t, h_{t-1}]W^{u} + b^u) \\\\\n",
    "&Reset \\: gate: \\qquad &\\Gamma^r=\\sigma(x_tW^{r} + h_{t-1}U^{r} + b^r) \\qquad \\rightarrow \\qquad &\\Gamma^r=\\sigma([x_t, h_{t-1}]W^{r} + b^r) \\\\\n",
    "&Candidate \\:cell: &\\tilde{h_t} = tanh([x_tW + (\\Gamma^r \\circ h_{t-1})U + b) \\qquad \\rightarrow \\qquad &\\tilde{h_t} = tanh([x_t, (\\Gamma^r \\circ h_{t-1})] W + b) \\\\\n",
    "&Final\\: cell:    &h_t = \\Gamma^u \\circ h_{t-1} + (1-\\Gamma^u) \\circ \\tilde{h_t} \\qquad \\rightarrow \\qquad &h_t = \\Gamma^u \\circ h_{t-1} + (1-\\Gamma^u) \\circ \\tilde{h_t}\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"weight_r\" matrix has correct shape\n",
      "The \"weight_u\" matrix has correct shape\n",
      "The \"weight\" matrix has correct shape\n",
      "The \"bias_r\" matrix has correct shape\n",
      "The \"bias_u\" matrix has correct shape\n",
      "The \"bias\" matrix has correct shape\n",
      "The \"weight_r\" matrix has correct values\n",
      "The \"weight_u\" matrix has correct values\n",
      "The \"weight\" matrix has correct values\n",
      "The \"bias_r\" matrix has correct values\n",
      "The \"bias_u\" matrix has correct values\n",
      "The \"bias\" matrix has correct values\n",
      "The \"forward\" function is implemented correctly\n"
     ]
    }
   ],
   "source": [
    "#Import modules\n",
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "import torch\n",
    "\n",
    "#Defining dummy variables\n",
    "hidden_state_sizes = 512\n",
    "inputSize  = 648\n",
    "batch_size = 128\n",
    "x          = torch.tensor(torch.rand(batch_size, inputSize))\n",
    "state_old  = torch.tensor(torch.rand(batch_size, hidden_state_sizes))\n",
    "\n",
    "# You should implement this function\n",
    "cell   = cocoSource.GRUCell(hidden_state_sizes, inputSize)\n",
    "cell_output = cell(x, state_old)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.GRUcell_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1c** \n",
    "\n",
    "Your task is to implement \"loss_fn\" in cocoSource.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumLoss has correct value\n",
      "meanLoss has correct value\n"
     ]
    }
   ],
   "source": [
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "import torch\n",
    "\n",
    "weight_dir = 'unit_tests/loss_fn_tensors.pt'\n",
    "checkpoint = torch.load(weight_dir)\n",
    "    \n",
    "logits     = checkpoint['logits']\n",
    "yTokens    = checkpoint['yTokens']\n",
    "yWeights   = checkpoint['yWeights']\n",
    "\n",
    "sumLoss, meanLoss = cocoSource.loss_fn(logits, yTokens, yWeights)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.loss_fn_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1d** \n",
    "\n",
    "You may want to have a look at \"utils/trainer.py\" and \"utils/model.py\" to understand the flow of the script.\n",
    "Your task is to implement the  RNN class in cocoSource.py. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'sourceFiles.cocoSource.RNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'sourceFiles.cocoSource.GRUCell' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- For is_train = True --------\n",
      "\"logits\" has correct shape\n",
      "\"logits\" has correct values\n",
      "\"current_hidden_state\" has correct shape\n",
      "\"current_hidden_state\" has correct values\n",
      "-------- For is_train = False --------\n",
      "\"logits\" has correct shape\n",
      "\"logits\" has incorrect values\n",
      "\"current_hidden_state\" has correct shape\n",
      "\"current_hidden_state\" has incorrect values\n"
     ]
    }
   ],
   "source": [
    "#Import modules\n",
    "\n",
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Config\n",
    "my_dir = 'unit_tests/RNN_tensors_is_train_False.pt'\n",
    "checkpoint = torch.load(my_dir)\n",
    "\n",
    "outputLayer   = checkpoint['outputLayer']\n",
    "Embedding     = checkpoint['Embedding']\n",
    "xTokens       = checkpoint['xTokens']\n",
    "initial_hidden_state  = checkpoint['initial_hidden_state']\n",
    "input_size            = checkpoint['input_size']\n",
    "hidden_state_size     = checkpoint['hidden_state_size']\n",
    "num_rnn_layers        = checkpoint['num_rnn_layers']\n",
    "cell_type             = checkpoint['cell_type']\n",
    "is_train              = checkpoint['is_train']\n",
    "\n",
    "# You should implement this function\n",
    "myRNN = cocoSource.RNN(input_size, hidden_state_size, num_rnn_layers, cell_type)\n",
    "logits, current_state = myRNN(xTokens, initial_hidden_state, outputLayer, Embedding, is_train)\n",
    "\n",
    "#Check implementation'\n",
    "is_train = True\n",
    "unit_tests.RNN_test(is_train)\n",
    "is_train = False\n",
    "unit_tests.RNN_test(is_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1e** \n",
    "\n",
    "You should now implement the imageCaptionModel class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- With \"current_hidden_state\"==None -----\n",
      "\"logits\" has correct shape\n",
      "\"logits\" has incorrect values\n",
      "\"current_hidden_state\" has correct shape\n",
      "\"current_hidden_state\" has incorrect values\n",
      " \n",
      "---- With \"current_hidden_state\"!=None -----\n",
      "cpu\n",
      "tmp type torch.FloatTensor\n",
      "cpu\n",
      "tmp type torch.FloatTensor\n",
      "cpu\n",
      "tmp type torch.FloatTensor\n",
      "cpu\n",
      "tmp type torch.FloatTensor\n",
      "cpu\n",
      "tmp type torch.FloatTensor\n",
      "cpu\n",
      "tmp type torch.FloatTensor\n",
      "cpu\n",
      "tmp type torch.FloatTensor\n",
      "\"logits\" has correct shape\n",
      "\"logits\" has correct values\n",
      "\"current_hidden_state\" has correct shape\n",
      "\"current_hidden_state\" has correct values\n"
     ]
    }
   ],
   "source": [
    "#Import modules\n",
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Config\n",
    "my_dir = 'unit_tests/imageCaptionModel_tensors.pt'\n",
    "checkpoint = torch.load(my_dir)\n",
    "config                            = checkpoint['config']\n",
    "vgg_fc7_features                  = checkpoint['vgg_fc7_features']\n",
    "xTokens                           = checkpoint['xTokens']\n",
    "is_train                          = checkpoint['is_train']\n",
    "myImageCaptionModelRef_state_dict = checkpoint['myImageCaptionModelRef_state_dict']\n",
    "logitsRef                         = checkpoint['logitsRef']\n",
    "current_hidden_state_Ref          = checkpoint['current_hidden_state_Ref']\n",
    "\n",
    "myImageCaptionModel = cocoSource.imageCaptionModel(config)\n",
    "logits, current_hidden_state = myImageCaptionModel(vgg_fc7_features, xTokens,  is_train)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.imageCaptionModel_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Task2'></a>\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "# Task 2: Train the image captioning network #\n",
    "\n",
    "\n",
    "Congratulations, you are done with all implemention. You shall now train various network architectures. The weights will be stored regularly, but updated when the validation loss has decrease only. When you are done training you can go to task 3 to start generating captions. \n",
    "\n",
    "Depending on your compute power, the training can take a long time. Start with small simple network architectures.\n",
    "\n",
    "The loss per epoch images will be stored in folder \"loss_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b319eaa4ff44cbae316ea38b44a3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=925), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n",
      "cuda:0\n",
      "tmp type torch.cuda.FloatTensor\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1683b7584bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# here you train your model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelParam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaveRestorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/shared/users/robersol/github/in5400/mandatory/m02/utils/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/users/robersol/github/in5400/mandatory/m02/utils/trainer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, mode, model, is_train, cur_epoch)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_hidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg_fc7_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTokens\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_hidden_state_Ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg_fc7_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTokens\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0msumLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeanLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/users/robersol/github/in5400/mandatory/m02/sourceFiles/cocoSource.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, vgg_fc7_features, xTokens, is_train, current_hidden_state)\u001b[0m\n\u001b[1;32m     90\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputLayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                                         \u001b[0mis_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                                         )\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/users/robersol/github/in5400/mandatory/m02/sourceFiles/cocoSource.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xTokens, initial_hidden_state, outputLayer, Embedding, is_train)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/users/robersol/github/in5400/mandatory/m02/sourceFiles/cocoSource.py\u001b[0m in \u001b[0;36m_train_forward\u001b[0;34m(self, xTokens, initial_hidden_state, outputLayer, Embedding, seqLen)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mcell_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.dataLoader import DataLoaderWrapper\n",
    "from utils.saverRestorer import SaverRestorer\n",
    "from utils.model import Model\n",
    "from utils.trainer import Trainer\n",
    "from utils.validate import plotImagesAndCaptions\n",
    "\n",
    "#Path if you work on personal computer\n",
    "#data_dir = 'data/coco/'\n",
    "\n",
    "#Path if you work on UIO IFI computer\n",
    "#data_dir = '/projects/in5400/oblig2/coco/'\n",
    "\n",
    "#Path if you work on one of the ML servers\n",
    "data_dir = '/shared/in5400/coco/'\n",
    "\n",
    "#train\n",
    "modelParam = {\n",
    "        'batch_size': 128,          # Training batch size\n",
    "        'cuda': {'use_cuda': True,  # Use_cuda=True: use GPU\n",
    "                 'device_idx': 0},  # Select gpu index: 0,1,2,3\n",
    "        'numbOfCPUThreadsUsed': 3,  # Number of cpu threads use in the dataloader\n",
    "        'numbOfEpochs': 20,         # Number of epochs\n",
    "        'data_dir': data_dir,       # data directory\n",
    "        'img_dir': 'loss_images/',\n",
    "        'modelsDir': 'storedModels/',\n",
    "        'modelName': 'model_0/',    # name of your trained model\n",
    "        'restoreModelLast': 0,\n",
    "        'restoreModelBest': 0,\n",
    "        'modeSetups':   [['train', True], ['val', True]],\n",
    "        'inNotebook': True,         # If running script in jupyter notebook\n",
    "        'inference': False\n",
    "}\n",
    "\n",
    "config = {\n",
    "        'optimizer': 'adam',             # 'SGD' | 'adam' | 'RMSprop'\n",
    "        'learningRate': {'lr': 0.0005},  # learning rate to the optimizer\n",
    "        'weight_decay': 0,               # weight_decay value\n",
    "        'VggFc7Size': 4096,              # Fixed, do not change\n",
    "        'embedding_size': 128,           # word embedding size\n",
    "        'vocabulary_size': 4000,        # number of different words\n",
    "        'truncated_backprop_length': 20,\n",
    "        'hidden_state_sizes': 20,       #\n",
    "        'num_rnn_layers': 2,             # number of stacked rnn's\n",
    "        'cellType': 'GRU'                # RNN or GRU\n",
    "        }\n",
    "\n",
    "\n",
    "# create an instance of the model you want\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    model = Model(config, modelParam)\n",
    "\n",
    "    # create an instacne of the saver and resoterer class\n",
    "    saveRestorer = SaverRestorer(config, modelParam)\n",
    "    model        = saveRestorer.restore(model)\n",
    "\n",
    "    # create your data generator\n",
    "    dataLoader = DataLoaderWrapper(config, modelParam)\n",
    "\n",
    "    # here you train your model\n",
    "    trainer = Trainer(model, modelParam, config, dataLoader, saveRestorer)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in model.net.rnn.cells:\n",
    "    print(c.weight.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Task3'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Task 3: Generate image captions on the validation set#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataLoader import DataLoaderWrapper\n",
    "from utils.saverRestorer import SaverRestorer\n",
    "from utils.model import Model\n",
    "from utils.trainer import Trainer\n",
    "from utils.validate import plotImagesAndCaptions\n",
    "\n",
    "#Path if you work on personal computer\n",
    "data_dir = 'data/coco/'\n",
    "\n",
    "#Path if you work on UIO IFI computer\n",
    "#data_dir = /projects/in5400/oblig2/coco/\n",
    "\n",
    "#Path if you work on one of the ML servers\n",
    "#data_dir = '/shared/in5400/coco/'\n",
    "\n",
    "modelParam = {\n",
    "        'batch_size': 128,          # Training batch size\n",
    "        'cuda': {'use_cuda': False,  # Use_cuda=True: use GPU\n",
    "                 'device_idx': 0},  # Select gpu index: 0,1,2,3\n",
    "        'numbOfCPUThreadsUsed': 3,  # Number of cpu threads use in the dataloader\n",
    "        'numbOfEpochs': 20,         # Number of epochs\n",
    "        'data_dir': data_dir,       # data directory\n",
    "        'img_dir': 'loss_images/',\n",
    "        'modelsDir': 'storedModels/',\n",
    "        'modelName': 'model_0/',    # name of your trained model\n",
    "        'restoreModelLast': 0,\n",
    "        'restoreModelBest': 0,\n",
    "        'modeSetups':   [['train', True], ['val', True]],\n",
    "        'inNotebook': True,         # If running script in jupyter notebook\n",
    "        'inference': True\n",
    "}\n",
    "\n",
    "config = {\n",
    "        'optimizer': 'adam',             # 'SGD' | 'adam' | 'RMSprop'\n",
    "        'learningRate': {'lr': 0.0005},  # learning rate to the optimizer\n",
    "        'weight_decay': 0,               # weight_decay value\n",
    "        'VggFc7Size': 4096,              # Fixed, do not change\n",
    "        'embedding_size': 128,           # word embedding size\n",
    "        'vocabulary_size': 4000,        # number of different words\n",
    "        'truncated_backprop_length': 20,\n",
    "        'hidden_state_sizes': 256,       #\n",
    "        'num_rnn_layers': 2,             # number of stacked rnn's\n",
    "        'cellType': 'RNN'                # RNN or GRU\n",
    "        }\n",
    "\n",
    "if modelParam['inference'] == True:\n",
    "    modelParam['batch_size'] = 1\n",
    "    modelParam['modeSetups'] = [['val', False]]\n",
    "    modelParam['restoreModelBest'] = 1\n",
    "\n",
    "# create an instance of the model you want\n",
    "model = Model(config, modelParam)\n",
    "\n",
    "# create an instacne of the saver and resoterer class\n",
    "saveRestorer = SaverRestorer(config, modelParam)\n",
    "model        = saveRestorer.restore(model)\n",
    "\n",
    "# create your data generator\n",
    "dataLoader = DataLoaderWrapper(config, modelParam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the cell below to generate captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotImagesAndCaptions\n",
    "plotImagesAndCaptions(model, modelParam, config, dataLoader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6 (Conda)",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
